[["what-is-eda.html", "Chapter 2 What is exploratory data analysis? 2.1 Current interpretations 2.2 Differentiating from traditional analysis 2.3 The origins of EDA 2.4 Exercises", " Chapter 2 What is exploratory data analysis? Data analysis is a process of cleaning, transforming, inspecting and modelling data with the aim of extracting information. Data analysis includes: exploratory data analysis, confirmatory data analysis, and initial data analysis. Before modelling and predicting, data should first be explored to uncover the patterns and structures that exist. Exploratory data analysis involves both numerical and visual techniques designed to reveal interesting information that may be hidden in the data. However, an analyst must be cautious not to over-interpret apparent patterns, and to make efforts assess the results of a data exploration are reliable for the data being studied and potentially for new data. In a confirmatory data analysis, the focus is on statistical inference and includes processes such as testing hypothesis, model selection, or predictive modelling. Initial data analysis is an important part of this, to check the sometimes strict assumptions made in order to conduct inference. 2.1 Current interpretations EDA today is prevalent, and there are many descriptions. It‚Äôs interesting to compare and contrast these, in relation to the original scope. From wikipedia In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. In Wickham and Grolemund (2017) EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends. The National Institute of Standards and Technology Exploratory Data Analysis (EDA) is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to (1) maximize insight into a data set; (2) uncover underlying structure; (3) extract important variables; (4) detect outliers and anomalies; (5) test underlying assumptions; (6) develop parsimonious models; and (7) determine optimal factor settings. The predictive analytics competition site Kaggle says What is Exploratory Data Analysis (EDA)? (1) How to ensure you are ready to use machine learning algorithms in a project? (2) How to choose the most suitable algorithms for your data set? (3) How to define the feature variables that can potentially be used for machine learning? Legacy data analysis software system SAS says EDA is necessary for the next stage of data research. If there was an analogy to exploratory data analysis, it would be that of a painter examining their tools and available time, before deciding on what best to paint. The massive online John Hopkins EDA class describes it as These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data. General information site Towards Data Science says The purpose of doing the Exploratory Data Analysis or EDA is to find new information in data. The understanding of EDA that practitioners may not aware of, is the EDA uses a visually-examined dataset to understand and summarize the main characteristics of the dataset without having a prior hypothesis or relying upon statistical models. 2.2 Differentiating from traditional analysis 2.2.1 From the case study In 1995 I (Di), as a newly fledged assistant professor, was asked to teach a Business Statistics course. In the process of looking for appropriate examples, I found a book titled ‚ÄúPractical Data Analysis: Case Studies in Business Statistics‚Äù by Bryant and Smith (Bryant and Smith 1995). The example on tipping in restaurants caught my eye, because I had always found tipping to be a difficult. Perhaps this was because I was an Australian new to the practice after moving to the USA for graduate school, in 1988 and still uncomfortable with the culture almost a decade later. The types of questions that bounce around mentally, should I tip because the wait staff‚Äôs wage comes mostly from this money, how much does the restaurant owner take from the worker, was the service good, have I just been paid and can be generous. What is tipping? According to The basic rules of tipping that everyone should know about: When you‚Äôre dining at a full-service restaurant Tip 20 percent of your full bill. When you grab a cup of coffee Round up or add a dollar if you‚Äôre a regular or ordered a complicated drink. When you have lunch at a food truck Drop a few dollars into the tip jar, but a little less than you would at a dine-in spot. When you use a gift card Tip on the total value of the meal, not just what you paid out of pocket. About the data The tips case study data is described as follows: In one restaurant, a food server recorded the following data on all customers they served during an interval of two and a half months in early 1990. Food servers‚Äô tips in restaurants may be influenced by many factors, including the nature of the restaurant, size of the party, and table locations in the restaurant. Restaurant managers need to know which factors matter when they assign tables to food servers. and a description of the variables is as reported in Cook and Swayne (2007): Case study procedure The analysis of the data should follow these steps according to the book: Step 1: Develop a model Should the response be tip alone and use the total bill as a predictor? Should you create a new variable tip rate and use this as the response? Step 2: Fit the full model with sex, smoker, day, time and size as predictors Step 3: Refine model: Should some variables should be dropped? Step 4: Check distribution of residuals Step 5: Summarise the model, if X=something, what would be the expected tip Step 1 Calculate tip % as tip/total bill \\(\\times\\) 100 library(tidyverse) tips &lt;- read_csv(&quot;http://ggobi.org/book/data/tips.csv&quot;) tips &lt;- tips %&gt;% mutate(tip_pct = tip/totbill * 100) #&lt;&lt; Step 2 Fit the full model with all variables tips_lm &lt;- tips %&gt;% select(tip_pct, sex, smoker, day, time, size) %&gt;% lm(tip_pct ~ ., data=.) #&lt;&lt; Summarise the model library(broom) library(kableExtra) tidy(tips_lm) %&gt;% #&lt;&lt; kable(digits=2) %&gt;% kable_styling() term estimate std.error statistic p.value (Intercept) 20.66 2.49 8.29 0.00 sexM -0.85 0.83 -1.02 0.31 smokerYes 0.36 0.85 0.43 0.67 daySat -0.18 1.83 -0.10 0.92 daySun 1.67 1.90 0.88 0.38 dayThu -1.82 2.32 -0.78 0.43 timeNight -2.34 2.61 -0.89 0.37 size -0.96 0.42 -2.28 0.02 glance(tips_lm) %&gt;% #&lt;&lt; select(r.squared, statistic, p.value) %&gt;% kable(digits=3) r.squared statistic p.value 0.042 1.479 0.175 ü§î Which variable(s) would be considered important for predicting tip %? Step 3 Refine the model. tips_lm &lt;- tips %&gt;% select(tip_pct, size) %&gt;% #&lt;&lt; lm(tip_pct ~ ., data=.) tidy(tips_lm) %&gt;% #&lt;&lt; kable(digits=2) %&gt;% kable_styling() term estimate std.error statistic p.value (Intercept) 18.44 1.12 16.47 0.00 size -0.92 0.41 -2.25 0.03 \\[\\widehat{tip %} = 18.44 - 0.92 \\times size\\] As the size of the dining party increases by one person the tip decreases by approximately 1%. glance(tips_lm) %&gt;% #&lt;&lt; select(r.squared, statistic, p.value) %&gt;% kable(digits=3) r.squared statistic p.value 0.02 5.042 0.026 \\(R^2 = 0.02\\). This dropped by half from the full model, even though no other variables contributed significantly to the model. It might be a good step to examine interaction terms. What does \\(R^2 = 0.02\\) mean? \\(R^2 = 0.02\\) means that size explains just 2% of the variance in tip %. This is a very weak model. And \\(R^2 = 0.04\\) is also a very weak model. What do the \\(F\\) statistic and \\(p\\)-value mean? Assume that we have a random sample from a population. Assume that the model for the population is \\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed \\[ \\widehat{tip %} = b_0 + b_1 sexM + ... + b_7 size \\] The \\(F\\) statistic refers to \\[ H_o: \\beta_1 = ... = \\beta_7 = 0 ~~ vs ~~ H_a: \\text{at least one is not 0}\\] The \\(p\\)-value is the probability that we observe the given \\(F\\) value or larger, computed assuming \\(H_o\\) is true. What do the \\(t\\) statistics and \\(p\\)-value associated with model coeficients mean? Assume that we have a random sample from a population. Assume that the model for the population is \\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed \\[ \\widehat{tip %} = b_0 + b_1 sexM + ... + b_7 size \\] The \\(t\\) statistics in the coefficient summary refer to \\[ H_o: \\beta_k = 0 ~~ vs ~~ H_a: \\beta_k \\neq 0 \\] The \\(p\\)-value is the probability that we observe the given \\(t\\) value or more extreme, computed assuming \\(H_o\\) is true. Model checking Normally, the final model summary would be accompanied diagnostic plots observed vs fitted values to check strength and appropriateness of the fit univariate plot, and normal probability plot, of residuals to check for normality in the simple final model like this, the observed vs predictor, with model overlaid would be advised to assess the model relative to the variability around the model when the final model has more terms, using a partial dependence plot to check the relative relationship between the response and predictors would be recommended. tips_aug &lt;- augment(tips_lm) ggplot(tips_aug, aes(x=.resid)) + #&lt;&lt; geom_histogram() + xlab(&quot;residuals&quot;) ggplot(tips_aug, aes(sample=.resid)) + #&lt;&lt; stat_qq() + stat_qq_line() + xlab(&quot;residuals&quot;) + theme(aspect.ratio=1) ggplot(tips_aug, aes(x=.fitted, y=tip_pct)) + #&lt;&lt; geom_point() + geom_smooth(method=&quot;lm&quot;) + xlab(&quot;observed&quot;) + ylab(&quot;fitted&quot;) ggplot(tips_aug, aes(x=size, y=tip_pct)) + #&lt;&lt; geom_point() + geom_smooth(method=&quot;lm&quot;) + ylab(&quot;tip %&quot;) The result of this work would leave us with a model that could be used to impose a dining/tipping policy in restaurants (see here) and should also leave us with an unease that this policy is based on weak support. 2.2.2 EDA approach It‚Äôs a good idea to examine the data description, and the explanation of the variables. What does that look like here? - tip and totbill are quantitative, but generally we would think about tip as a percentage of the bill, so creating this new variable would be useful, too. - sex, smoker, day, time of day are categorical. - size is discrete/integer. The types of variables suggest what summaries to make. For the quantitative variables we would calculate - means, standard deviations, median, quartiles, minimum and maximum, and - also make histograms or density plots and scatterplots to explore bivariate relationships. For categorical variables, and integer variables with few categories, compute - counts and proportions, and - make bar charts and mosaic plots Thinking about relationships between categorical and quantitative variables, we could - calculate the numerical summaries by the categorical variable levels, and - make histograms and scatterplots, facetted by the categorical variables. Look at the distribution of tips, total bill. ggplot(tips, aes(x=tip)) + #&lt;&lt; geom_histogram( colour=&quot;white&quot;) Because, one binwidth is never enough ‚Ä¶ ggplot(tips, aes(x=tip)) + geom_histogram( breaks=seq(0.5,10.5,1), #&lt;&lt; colour=&quot;white&quot;) + scale_x_continuous( breaks=seq(0,11,1)) Big fat bins. Tips are skewed, which means most tips are relatively small. ggplot(tips, aes(x=tip)) + geom_histogram( breaks=seq(0.5,10.5,0.1), #&lt;&lt; colour=&quot;white&quot;) + scale_x_continuous( breaks=seq(0,11,1)) Skinny bins. Tips are multimodal, and occurring at the full dollar and 50c amounts. Relationship between tip and total p &lt;- ggplot(tips, aes(x=totbill, y=tip)) + geom_point() + #&lt;&lt; scale_y_continuous( breaks=seq(0,11,1)) p Adding a regression line, helps to focus on a tipping standard, and whether individual tips are above or below expected. p &lt;- p + geom_abline(intercept=0, #&lt;&lt; slope=0.2) + #&lt;&lt; annotate(&quot;text&quot;, x=45, y=10, label=&quot;20% tip&quot;) p Most tips less than 20%, there are more ‚Äúskin flints‚Äù than generous diners. There are a couple of big tips, and we can see the banding horizontally that is the rounding seen from the histogram of tips. Examine the distributions across categorical variables. p + facet_grid(smoker~sex) #&lt;&lt; The bigger bills tend to be paid by men (and females that smoke). Except for three diners, female non-smokers are very consistent tippers, probably around 15-18% though. The variability in the smokers is much higher than for the non-smokers. Isn‚Äôt this interesting? 2.2.3 Summarising the difference In the above example we gained a wealth of insight in a short time. Using nothing but graphical methods we investigated univariate, bivariate, and multivariate relationships. We found both global features and local detail. We saw that tips were rounded; then we saw the obvious correlation between the tip and the size of the bill, noting the scarcity of generous tippers; finally we discovered differences in the tipping behavior of male and female smokers and non-smokers. Notice that we used very simple plots to explore some pretty complex relationships involving as many as four variables. Each plot shows a subset obtained by partitioning the data according to two binary variables. The statistical term for partitioning based on variables is ‚Äúconditioning.‚Äù For example, the top left plot shows the dining parties that meet the condition that the bill payer was a male non-smoker: sex = male and smoking = False. In database terminology this plot would be called the result of ‚Äúdrill-down.‚Äù The idea of conditioning is richer than drill-down because it involves a structured partitioning of all data as opposed to the extraction of a single partition. Having generated the four plots, we arrange them in a two-by-two layout to reflect the two variables on which we conditioned. Although the axes in each plot are tip and bill, the axes of the overall figure are smoking (vertical) and sex (horizontal). The arrangement permits us to make several kinds of comparisons and to make observations about the partitions. 2.2.4 Reality check The preceding explanations may have given a somewhat misleading impression of the process of data analysis. In our account the data had no problems; for example, there were no missing values and no recording errors. Every step was logical and necessary. Every question we asked had a meaningful answer. Every plot that was produced was useful and informative. In actual data analysis, nothing could be further from the truth. Real datasets are rarely perfect; most choices are guided by intuition, knowledge, and judgment; most steps lead to dead ends; most plots end up in the wastebasket. This may sound daunting, but even though data analysis is a highly improvisational activity, it can be given some structure nonetheless. 2.2.5 What is EDA? In this stage in the analysis, we make time to ‚Äúplay in the sand‚Äù to allow us to find the unexpected, and come to some understanding of our data. We like to think of this as a little like traveling. We may have a purpose in visiting a new city, perhaps to attend a conference, but we need to take care of our basic necessities, such as finding eating places and gas stations. Some of our movements will be pre-determined, or guided by the advice of others, but some of the time we wander around by ourselves. We may find a cafe we particularly like or a cheaper gas station. This is all about getting to know the neighborhood. EDA has always depended heavily on graphics, even before the term data visualization was coined. Our favorite quote from John Tukey‚Äôs rich legacy is that we need good pictures to ‚Äúforce the unexpected upon us.‚Äù 2.2.6 Isn‚Äôt it data snooping? Because EDA is very graphical, it sometimes gives rise to a suspicion that patterns in the data are being detected and reported that are not really there. So many different combinations may be examined, that something is bound to be interesting. ‚ò†Ô∏è Structure seen in the plot drives hypothesis testing (on same data). Sometimes this is called data snooping. We snooped into the tips data, and from a few plots we learned an enormous amount of information about tipping: There is a scarcity of generous tippers, the variability in tips increases extraordinarily for smoking parties, and people tend to round their tips. These are very different types of tipping behaviors than what we learned from the regression model. The regression model was not compromised by what we learned from graphics, and indeed, we have a richer and more informative analysis. Making plots of the data is just smart. False discovery is the lesser danger when compared to non-discovery. Non-discovery is the failure to identify meaningful structure, and it may result in false or incomplete modeling. In a healthy scientific enterprise, the fear of non-discovery should be at least as great as the fear of false discovery. 2.2.7 Why aren‚Äôt there more resources on EDA? Teaching data analysis is not easy, and the time allowed is always far from sufficient. But these difficulties have been enhanced by the view that ‚Äúavoidance of cookbookery and growth of understanding come only by mathematical treatment, with emphasis upon proofs.‚Äù The problem of cookbookery is not peculiar to data analysis. But the solution of concentrating upon mathematics and proof is. Tukey (1962) There are many courses now, every introductory statistics course begins with exploratory data analysis, and teaches box plots. It is just a simple treatment, though. A book by Peng, and a Coursera class by Peng, Leek and Caffo with more than a 100,000 regularly enrolled. But we argue that there is something missing from all of this. Almost all of these resources focus on what would be called initial data analysis. These don‚Äôt capture the spirit of EDA, as introduced by Tukey. 2.3 The origins of EDA The field of exploratory data analysis came of age with the appearance of (tukey99?). Tukey held that too much emphasis in statistics was placed on statistical hypothesis testing (confirmatory data analysis); more emphasis needed to be placed on using data to suggest hypotheses to test. Take a glimpse back in time is possible with the American Statistical Association video lending library. Watch John Tukey talking about exploring high-dimensional data with an amazing new computer in 1973, four years before the EDA book. Look out for these things: Tukey‚Äôs expertise is described as for trial and error learning and the computing equipment, especially how the human interacted with the screen, and the hair style of Jerry Friedman. The first 4.25 minutes are the best! 2.3.1 Guiding principles It is important to understand what you CAN DO before you learn to measure how WELL you seem to have DONE it. Learning first what you can do will help you to work more easily and effectively. The book does not exist to make the case that exploratory data analysis is useful. Rather it exists to expose its readers and users to a considerable variety of techniques for looking more effectively at one‚Äôs data. The examples are not intended to be complete case histories. Rather they should isolated techniques in action on real data. The emphasis is on general techniques, rather than specific problems. A basic problem about any body of data is to make it more easily and effectively handleable by minds ‚Äì our minds, her mind, his mind. To this general end: anything that make a simpler description possible makes the description more easily handleable. anything that looks below the previously described surface makes the description more effective. So we shall always be glad (a) to simplify description and (b) to describe one layer deeper. In particular, to be able to say that we looked one layer deeper, and found nothing, is a definite step forward ‚Äì though not as far as to be able to say that we looked deeper and found thus-and-such. to be able to say that ‚Äúif we change our point of view in the following way ‚Ä¶ things are simpler‚Äù is always a gain‚Äìthough not quite so much as to be able to say ‚Äúif we don‚Äôt bother to change out point of view (some other) things are equally simple.‚Äù Consistent with this view, we believe, is a clear demand that pictures based on exploration of data should force their messages upon us. Pictures that emphasize what we already know‚Äì‚Äúsecurity blankets‚Äù to reassure us‚Äìare frequently not worth the space they take. Pictures that have to be gone over with a reading glass to see the main point are wasteful of time and inadequate of effect. The greatest value of a picture is when it forces us to notice what we never expected to see. Once upon a time, statisticians only explored. Then they learned to confirm exactly‚Äìto confirm a few things exactly, each under very specific circumstances. As they emphasized exact confirmation, their techniques inevitably became less flexible. The connection of the most used techniques with past insights was weakened. Anything to which confirmatory procedure was not explicitly attached was decried as ‚Äúmere descriptive statistics,‚Äù no matter how much we learned from it. Today, the flexibility of (approximate) confirmation by the jacknife (add simulation, bootstrap and permutation now) makes it relatively easy to ask, for almost any clearly specified exploration, ‚ÄúHow far is it confirmed?‚Äù Today, exploratory and confirmatory can‚Äìand should‚Äìproceed side by side. both teacher and student need to realise that many problems do not have a single ‚Äúright answer.‚Äù There can be many ways to approach a body of data. Not all are equally good. For some bodies of data this may be clear, but for others we may not be able to tell from a single body of data which approach is preferred. Even several bodies of data about very similar situations may not be enough to show which approach should be preferred. Accordingly, it will often be quite reasonable for different analysts to reach somewhat different analyses. Yet more‚Äìto unlock the analysis of a body of day, to find the good way to approach it, may require a key, whose finding is a creative act. Not everyone can be expected to create the key to any one situation. And to continue to paraphrase Barnum, no one can be expected to create a key to each situation he or she meets. To learn about data analysis, it is right that each of us try many things that do not work‚Äìthat we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed‚Äìat least until we were told about it‚Äìan opportunity to learn more. Each teacher needs to recognize this in grading and commenting on problems. The teacher who heeds these words and admits that there need be no one correct approach may, I regret to contemplate, still want whatever is done to be digit perfect. (Under such a requirement, the write should still be able to pass the course, but it is not clear whether she would get an ‚ÄúA.‚Äù) One does, from time to time, have to produce digit-perfect, carefully checked results, but forgiving techniques that are not too distributed by unusual data are also, usually, little disturbed by SMALL arithmetic errors. The techniques we discuss here have been chosen to be forgiving. It is hoped, then, that small arithmetic errors will take little off the problem‚Äôs grades, leaving severe penalties for larger errors, either of arithmetic or concept. Everything in Tukey‚Äôs EDA book could be done with pencil and paper. 2.3.2 Scratching down numbers Prices of Chevrolet in the local used car newspaper ads of 1968. Scratch down these numbers to make a stem-and-leaf plot, one of Tukey‚Äôs contributions that can still be seen introductory statistics books. Split the numbers between the 100\\(^{th}\\) and 10\\(^{th}\\) place digits. The left digits form the step and the right form the leaves of a stem-and-leaf plot. We will also drop the last digit, lose some precision but gain simplicity, making it easier to get an overall sense of the pattern of numbers. Thus 250 becomes 2 5, and 1099 becomes 10 9. The range of the stem will run from 2 through 18. Write these numbers from smallest to largest vertical, keeping digits in place. Add a vertical line to the right of the numbers. Now work through the full set of numbers, writing the last digit against it‚Äôs corresponding stem. The last steps is to re-draw it so that the leaves are ordered from lowest (left) to highest on each stem. And finally, think about the pattern of the numbers in the display. You should see that there are several clumps of numbers in this data. Wasn‚Äôt this fun! 2.3.3 Schematic summary 2.3.4 Easy re-expression 2.3.5 Effective comparison 2.3.6 Plots of relationship 2.4 Exercises Make a histogram of the total bill. Is there a similar pattern of multimodality? In the scatterplot of tip vs total bill, why is total on the horizontal axis? Make a stem-and-leaf plot of these numbers. References "]]
